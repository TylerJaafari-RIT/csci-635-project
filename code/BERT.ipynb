{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "d16cbe90-5016-410d-bd67-022a4f64a01f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizerFast, BertForSequenceClassification\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "from transformers import pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "011d92aa-3190-4822-907a-b96fef255ac1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Tweet Id</th>\n",
       "      <th>Entity</th>\n",
       "      <th>Sentiment</th>\n",
       "      <th>Tweet Content</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2401</td>\n",
       "      <td>Borderlands</td>\n",
       "      <td>Positive</td>\n",
       "      <td>im getting on borderlands and i will murder yo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2401</td>\n",
       "      <td>Borderlands</td>\n",
       "      <td>Positive</td>\n",
       "      <td>I am coming to the borders and I will kill you...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2401</td>\n",
       "      <td>Borderlands</td>\n",
       "      <td>Positive</td>\n",
       "      <td>im getting on borderlands and i will kill you ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2401</td>\n",
       "      <td>Borderlands</td>\n",
       "      <td>Positive</td>\n",
       "      <td>im coming on borderlands and i will murder you...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2401</td>\n",
       "      <td>Borderlands</td>\n",
       "      <td>Positive</td>\n",
       "      <td>im getting on borderlands 2 and i will murder ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74677</th>\n",
       "      <td>9200</td>\n",
       "      <td>Nvidia</td>\n",
       "      <td>Positive</td>\n",
       "      <td>Just realized that the Windows partition of my...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74678</th>\n",
       "      <td>9200</td>\n",
       "      <td>Nvidia</td>\n",
       "      <td>Positive</td>\n",
       "      <td>Just realized that my Mac window partition is ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74679</th>\n",
       "      <td>9200</td>\n",
       "      <td>Nvidia</td>\n",
       "      <td>Positive</td>\n",
       "      <td>Just realized the windows partition of my Mac ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74680</th>\n",
       "      <td>9200</td>\n",
       "      <td>Nvidia</td>\n",
       "      <td>Positive</td>\n",
       "      <td>Just realized between the windows partition of...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74681</th>\n",
       "      <td>9200</td>\n",
       "      <td>Nvidia</td>\n",
       "      <td>Positive</td>\n",
       "      <td>Just like the windows partition of my Mac is l...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>74682 rows Ã— 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       Tweet Id       Entity Sentiment  \\\n",
       "0          2401  Borderlands  Positive   \n",
       "1          2401  Borderlands  Positive   \n",
       "2          2401  Borderlands  Positive   \n",
       "3          2401  Borderlands  Positive   \n",
       "4          2401  Borderlands  Positive   \n",
       "...         ...          ...       ...   \n",
       "74677      9200       Nvidia  Positive   \n",
       "74678      9200       Nvidia  Positive   \n",
       "74679      9200       Nvidia  Positive   \n",
       "74680      9200       Nvidia  Positive   \n",
       "74681      9200       Nvidia  Positive   \n",
       "\n",
       "                                           Tweet Content  \n",
       "0      im getting on borderlands and i will murder yo...  \n",
       "1      I am coming to the borders and I will kill you...  \n",
       "2      im getting on borderlands and i will kill you ...  \n",
       "3      im coming on borderlands and i will murder you...  \n",
       "4      im getting on borderlands 2 and i will murder ...  \n",
       "...                                                  ...  \n",
       "74677  Just realized that the Windows partition of my...  \n",
       "74678  Just realized that my Mac window partition is ...  \n",
       "74679  Just realized the windows partition of my Mac ...  \n",
       "74680  Just realized between the windows partition of...  \n",
       "74681  Just like the windows partition of my Mac is l...  \n",
       "\n",
       "[74682 rows x 4 columns]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Loading data\n",
    "train_set = pd.read_csv(\"../data/twitter_training.csv\",names=[\"Tweet Id\",\"Entity\",\"Sentiment\",\"Tweet Content\"])\n",
    "train_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b714e6b7-c5bd-4393-a47d-68236279fbd9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dropped 61\n",
      "dropped 553\n",
      "dropped 589\n",
      "dropped 745\n",
      "dropped 1105\n",
      "dropped 1106\n",
      "dropped 2295\n",
      "dropped 2296\n",
      "dropped 2297\n",
      "dropped 2359\n",
      "dropped 2413\n",
      "dropped 2473\n",
      "dropped 2474\n",
      "dropped 2929\n",
      "dropped 2983\n",
      "dropped 2997\n",
      "dropped 2998\n",
      "dropped 2999\n",
      "dropped 3061\n",
      "dropped 3097\n",
      "dropped 3243\n",
      "dropped 3244\n",
      "dropped 3245\n",
      "dropped 3337\n",
      "dropped 3457\n",
      "dropped 3697\n",
      "dropped 3835\n",
      "dropped 3836\n",
      "dropped 3939\n",
      "dropped 3940\n",
      "dropped 3941\n",
      "dropped 3967\n",
      "dropped 4233\n",
      "dropped 4234\n",
      "dropped 4235\n",
      "dropped 4237\n",
      "dropped 4413\n",
      "dropped 4414\n",
      "dropped 4415\n",
      "dropped 4755\n",
      "dropped 4756\n",
      "dropped 4757\n",
      "dropped 4869\n",
      "dropped 4870\n",
      "dropped 4871\n",
      "dropped 4899\n",
      "dropped 4900\n",
      "dropped 4901\n",
      "dropped 5421\n",
      "dropped 5422\n",
      "dropped 5423\n",
      "dropped 5521\n",
      "dropped 5559\n",
      "dropped 5560\n",
      "dropped 5561\n",
      "dropped 5731\n",
      "dropped 5973\n",
      "dropped 5974\n",
      "dropped 5975\n",
      "dropped 6015\n",
      "dropped 6016\n",
      "dropped 6017\n",
      "dropped 6067\n",
      "dropped 6068\n",
      "dropped 6081\n",
      "dropped 6082\n",
      "dropped 6083\n",
      "dropped 6585\n",
      "dropped 6586\n",
      "dropped 6587\n",
      "dropped 6699\n",
      "dropped 6700\n",
      "dropped 6701\n",
      "dropped 6861\n",
      "dropped 6862\n",
      "dropped 6863\n",
      "dropped 6903\n",
      "dropped 6904\n",
      "dropped 6905\n",
      "dropped 7051\n",
      "dropped 7305\n",
      "dropped 7306\n",
      "dropped 7307\n",
      "dropped 7669\n",
      "dropped 7809\n",
      "dropped 7810\n",
      "dropped 7811\n",
      "dropped 8407\n",
      "dropped 8691\n",
      "dropped 8692\n",
      "dropped 8693\n",
      "dropped 8877\n",
      "dropped 8878\n",
      "dropped 8879\n",
      "dropped 9013\n",
      "dropped 9253\n",
      "dropped 9254\n",
      "dropped 9453\n",
      "dropped 9454\n",
      "dropped 9455\n",
      "dropped 9495\n",
      "dropped 9496\n",
      "dropped 9497\n",
      "dropped 9561\n",
      "dropped 9562\n",
      "dropped 9563\n",
      "dropped 9703\n",
      "dropped 9931\n",
      "dropped 9932\n",
      "dropped 9987\n",
      "dropped 9988\n",
      "dropped 9989\n",
      "dropped 10035\n",
      "dropped 10036\n",
      "dropped 10037\n",
      "dropped 10143\n",
      "dropped 10144\n",
      "dropped 10145\n",
      "dropped 10255\n",
      "dropped 10263\n",
      "dropped 10264\n",
      "dropped 10265\n",
      "dropped 10419\n",
      "dropped 10420\n",
      "dropped 10421\n",
      "dropped 10435\n",
      "dropped 10485\n",
      "dropped 10486\n",
      "dropped 10487\n",
      "dropped 10551\n",
      "dropped 10552\n",
      "dropped 10553\n",
      "dropped 10677\n",
      "dropped 10678\n",
      "dropped 10679\n",
      "dropped 10825\n",
      "dropped 11325\n",
      "dropped 11326\n",
      "dropped 11327\n",
      "dropped 11335\n",
      "dropped 11463\n",
      "dropped 11464\n",
      "dropped 11465\n",
      "dropped 11547\n",
      "dropped 11548\n",
      "dropped 11549\n",
      "dropped 11557\n",
      "dropped 11558\n",
      "dropped 12169\n",
      "dropped 12331\n",
      "dropped 12487\n",
      "dropped 12561\n",
      "dropped 12562\n",
      "dropped 12563\n",
      "dropped 13249\n",
      "dropped 13441\n",
      "dropped 13921\n",
      "dropped 14492\n",
      "dropped 14545\n",
      "dropped 14546\n",
      "dropped 14557\n",
      "dropped 16225\n",
      "dropped 16429\n",
      "dropped 16483\n",
      "dropped 16593\n",
      "dropped 16594\n",
      "dropped 16595\n",
      "dropped 16807\n",
      "dropped 16903\n",
      "dropped 17107\n",
      "dropped 17373\n",
      "dropped 17374\n",
      "dropped 17375\n",
      "dropped 17623\n",
      "dropped 17667\n",
      "dropped 17668\n",
      "dropped 17669\n",
      "dropped 18127\n",
      "dropped 18151\n",
      "dropped 18242\n",
      "dropped 18301\n",
      "dropped 19177\n",
      "dropped 19381\n",
      "dropped 20463\n",
      "dropped 20464\n",
      "dropped 20465\n",
      "dropped 20623\n",
      "dropped 20624\n",
      "dropped 21079\n",
      "dropped 21265\n",
      "dropped 21799\n",
      "dropped 21901\n",
      "dropped 22035\n",
      "dropped 22036\n",
      "dropped 22037\n",
      "dropped 22119\n",
      "dropped 22120\n",
      "dropped 22121\n",
      "dropped 22399\n",
      "dropped 22489\n",
      "dropped 22555\n",
      "dropped 22575\n",
      "dropped 22576\n",
      "dropped 22577\n",
      "dropped 22705\n",
      "dropped 23079\n",
      "dropped 23080\n",
      "dropped 23081\n",
      "dropped 23349\n",
      "dropped 23350\n",
      "dropped 23351\n",
      "dropped 23523\n",
      "dropped 23524\n",
      "dropped 23525\n",
      "dropped 23541\n",
      "dropped 23542\n",
      "dropped 23543\n",
      "dropped 23599\n",
      "dropped 23707\n",
      "dropped 23708\n",
      "dropped 24067\n",
      "dropped 24343\n",
      "dropped 24631\n",
      "dropped 24632\n",
      "dropped 25147\n",
      "dropped 25148\n",
      "dropped 25233\n",
      "dropped 25234\n",
      "dropped 25235\n",
      "dropped 25239\n",
      "dropped 25240\n",
      "dropped 25241\n",
      "dropped 25809\n",
      "dropped 25810\n",
      "dropped 25811\n",
      "dropped 26551\n",
      "dropped 27087\n",
      "dropped 27088\n",
      "dropped 27089\n",
      "dropped 27705\n",
      "dropped 27706\n",
      "dropped 27707\n",
      "dropped 27927\n",
      "dropped 27928\n",
      "dropped 27929\n",
      "dropped 28015\n",
      "dropped 28077\n",
      "dropped 28078\n",
      "dropped 28079\n",
      "dropped 28135\n",
      "dropped 28227\n",
      "dropped 28228\n",
      "dropped 28229\n",
      "dropped 28279\n",
      "dropped 28353\n",
      "dropped 28354\n",
      "dropped 28355\n",
      "dropped 28975\n",
      "dropped 29847\n",
      "dropped 29848\n",
      "dropped 29849\n",
      "dropped 29983\n",
      "dropped 30255\n",
      "dropped 30256\n",
      "dropped 30257\n",
      "dropped 30331\n",
      "dropped 30721\n",
      "dropped 30729\n",
      "dropped 30730\n",
      "dropped 30731\n",
      "dropped 31791\n",
      "dropped 31792\n",
      "dropped 31793\n",
      "dropped 32319\n",
      "dropped 32320\n",
      "dropped 32321\n",
      "dropped 32565\n",
      "dropped 32566\n",
      "dropped 32567\n",
      "dropped 32577\n",
      "dropped 32578\n",
      "dropped 32579\n",
      "dropped 33231\n",
      "dropped 33232\n",
      "dropped 33233\n",
      "dropped 33291\n",
      "dropped 33292\n",
      "dropped 33293\n",
      "dropped 33343\n",
      "dropped 33405\n",
      "dropped 33406\n",
      "dropped 33407\n",
      "dropped 33417\n",
      "dropped 33418\n",
      "dropped 33419\n",
      "dropped 33445\n",
      "dropped 33879\n",
      "dropped 33880\n",
      "dropped 33881\n",
      "dropped 34149\n",
      "dropped 34150\n",
      "dropped 34151\n",
      "dropped 34333\n",
      "dropped 34411\n",
      "dropped 34845\n",
      "dropped 34846\n",
      "dropped 34847\n",
      "dropped 35031\n",
      "dropped 35032\n",
      "dropped 35033\n",
      "dropped 35169\n",
      "dropped 35170\n",
      "dropped 35171\n",
      "dropped 35541\n",
      "dropped 35542\n",
      "dropped 35543\n",
      "dropped 35671\n",
      "dropped 35685\n",
      "dropped 35686\n",
      "dropped 35687\n",
      "dropped 35907\n",
      "dropped 35908\n",
      "dropped 35909\n",
      "dropped 35967\n",
      "dropped 35968\n",
      "dropped 35969\n",
      "dropped 36183\n",
      "dropped 36184\n",
      "dropped 36185\n",
      "dropped 36241\n",
      "dropped 36307\n",
      "dropped 36567\n",
      "dropped 36568\n",
      "dropped 36569\n",
      "dropped 36903\n",
      "dropped 36904\n",
      "dropped 36905\n",
      "dropped 36927\n",
      "dropped 36928\n",
      "dropped 36929\n",
      "dropped 37083\n",
      "dropped 37084\n",
      "dropped 37085\n",
      "dropped 37215\n",
      "dropped 37216\n",
      "dropped 37217\n",
      "dropped 37483\n",
      "dropped 37747\n",
      "dropped 37969\n",
      "dropped 37999\n",
      "dropped 38499\n",
      "dropped 38500\n",
      "dropped 38501\n",
      "dropped 38593\n",
      "dropped 38635\n",
      "dropped 39027\n",
      "dropped 39028\n",
      "dropped 39029\n",
      "dropped 39669\n",
      "dropped 39670\n",
      "dropped 39671\n",
      "dropped 39897\n",
      "dropped 39898\n",
      "dropped 39899\n",
      "dropped 40125\n",
      "dropped 40126\n",
      "dropped 40127\n",
      "dropped 40251\n",
      "dropped 40252\n",
      "dropped 40253\n",
      "dropped 40345\n",
      "dropped 40357\n",
      "dropped 40377\n",
      "dropped 40378\n",
      "dropped 40379\n",
      "dropped 40809\n",
      "dropped 40810\n",
      "dropped 40811\n",
      "dropped 41367\n",
      "dropped 41368\n",
      "dropped 41369\n",
      "dropped 41545\n",
      "dropped 41805\n",
      "dropped 41806\n",
      "dropped 41807\n",
      "dropped 41811\n",
      "dropped 41812\n",
      "dropped 41813\n",
      "dropped 42165\n",
      "dropped 42166\n",
      "dropped 42167\n",
      "dropped 42225\n",
      "dropped 42226\n",
      "dropped 42227\n",
      "dropped 42351\n",
      "dropped 42352\n",
      "dropped 42353\n",
      "dropped 42517\n",
      "dropped 42518\n",
      "dropped 42603\n",
      "dropped 42604\n",
      "dropped 42605\n",
      "dropped 42813\n",
      "dropped 42814\n",
      "dropped 42815\n",
      "dropped 42867\n",
      "dropped 42868\n",
      "dropped 42869\n",
      "dropped 42969\n",
      "dropped 42970\n",
      "dropped 42971\n",
      "dropped 43407\n",
      "dropped 43408\n",
      "dropped 43409\n",
      "dropped 43447\n",
      "dropped 43537\n",
      "dropped 43633\n",
      "dropped 43683\n",
      "dropped 43684\n",
      "dropped 43685\n",
      "dropped 43747\n",
      "dropped 43777\n",
      "dropped 43929\n",
      "dropped 43930\n",
      "dropped 43931\n",
      "dropped 43965\n",
      "dropped 43966\n",
      "dropped 43967\n",
      "dropped 44449\n",
      "dropped 44649\n",
      "dropped 44650\n",
      "dropped 44651\n",
      "dropped 44857\n",
      "dropped 45061\n",
      "dropped 45133\n",
      "dropped 45403\n",
      "dropped 45537\n",
      "dropped 45538\n",
      "dropped 45539\n",
      "dropped 45711\n",
      "dropped 45712\n",
      "dropped 45713\n",
      "dropped 46071\n",
      "dropped 46072\n",
      "dropped 46073\n",
      "dropped 47019\n",
      "dropped 47020\n",
      "dropped 47021\n",
      "dropped 47431\n",
      "dropped 47599\n",
      "dropped 47803\n",
      "dropped 48043\n",
      "dropped 48081\n",
      "dropped 48082\n",
      "dropped 48083\n",
      "dropped 48253\n",
      "dropped 48343\n",
      "dropped 48573\n",
      "dropped 48574\n",
      "dropped 48575\n",
      "dropped 48753\n",
      "dropped 48754\n",
      "dropped 48755\n",
      "dropped 48949\n",
      "dropped 48950\n",
      "dropped 49489\n",
      "dropped 50181\n",
      "dropped 50182\n",
      "dropped 50183\n",
      "dropped 50515\n",
      "dropped 50631\n",
      "dropped 50632\n",
      "dropped 50633\n",
      "dropped 50643\n",
      "dropped 50644\n",
      "dropped 50645\n",
      "dropped 50745\n",
      "dropped 50746\n",
      "dropped 50747\n",
      "dropped 51453\n",
      "dropped 51454\n",
      "dropped 51455\n",
      "dropped 51853\n",
      "dropped 52249\n",
      "dropped 52855\n",
      "dropped 52977\n",
      "dropped 52978\n",
      "dropped 52979\n",
      "dropped 53121\n",
      "dropped 53122\n",
      "dropped 53123\n",
      "dropped 53503\n",
      "dropped 53859\n",
      "dropped 53860\n",
      "dropped 53861\n",
      "dropped 53899\n",
      "dropped 54045\n",
      "dropped 54046\n",
      "dropped 54047\n",
      "dropped 54111\n",
      "dropped 54112\n",
      "dropped 54113\n",
      "dropped 54739\n",
      "dropped 55035\n",
      "dropped 55036\n",
      "dropped 55037\n",
      "dropped 55497\n",
      "dropped 55498\n",
      "dropped 55499\n",
      "dropped 55681\n",
      "dropped 56035\n",
      "dropped 56043\n",
      "dropped 56044\n",
      "dropped 56045\n",
      "dropped 56077\n",
      "dropped 56619\n",
      "dropped 56620\n",
      "dropped 56621\n",
      "dropped 56683\n",
      "dropped 56733\n",
      "dropped 56734\n",
      "dropped 56735\n",
      "dropped 56739\n",
      "dropped 56740\n",
      "dropped 56741\n",
      "dropped 56907\n",
      "dropped 56908\n",
      "dropped 56909\n",
      "dropped 57183\n",
      "dropped 57184\n",
      "dropped 57185\n",
      "dropped 57325\n",
      "dropped 57355\n",
      "dropped 57681\n",
      "dropped 57682\n",
      "dropped 57683\n",
      "dropped 57795\n",
      "dropped 57796\n",
      "dropped 57797\n",
      "dropped 57837\n",
      "dropped 57838\n",
      "dropped 57839\n",
      "dropped 58105\n",
      "dropped 58161\n",
      "dropped 58162\n",
      "dropped 58163\n",
      "dropped 58731\n",
      "dropped 58732\n",
      "dropped 58733\n",
      "dropped 59043\n",
      "dropped 59044\n",
      "dropped 59045\n",
      "dropped 59125\n",
      "dropped 60356\n",
      "dropped 61081\n",
      "dropped 61165\n",
      "dropped 62007\n",
      "dropped 62008\n",
      "dropped 62009\n",
      "dropped 62025\n",
      "dropped 62026\n",
      "dropped 62027\n",
      "dropped 62371\n",
      "dropped 62372\n",
      "dropped 62809\n",
      "dropped 63105\n",
      "dropped 63106\n",
      "dropped 63107\n",
      "dropped 63445\n",
      "dropped 63591\n",
      "dropped 63592\n",
      "dropped 63593\n",
      "dropped 63639\n",
      "dropped 63640\n",
      "dropped 63641\n",
      "dropped 63769\n",
      "dropped 63933\n",
      "dropped 63934\n",
      "dropped 63935\n",
      "dropped 63969\n",
      "dropped 63970\n",
      "dropped 63971\n",
      "dropped 64167\n",
      "dropped 64168\n",
      "dropped 64169\n",
      "dropped 64569\n",
      "dropped 64570\n",
      "dropped 64571\n",
      "dropped 65779\n",
      "dropped 66457\n",
      "dropped 67375\n",
      "dropped 67681\n",
      "dropped 68067\n",
      "dropped 68068\n",
      "dropped 68069\n",
      "dropped 68169\n",
      "dropped 68170\n",
      "dropped 68171\n",
      "dropped 68367\n",
      "dropped 68368\n",
      "dropped 68369\n",
      "dropped 68389\n",
      "dropped 68451\n",
      "dropped 68452\n",
      "dropped 68453\n",
      "dropped 68679\n",
      "dropped 68680\n",
      "dropped 68681\n",
      "dropped 68901\n",
      "dropped 68902\n",
      "dropped 68903\n",
      "dropped 69093\n",
      "dropped 69094\n",
      "dropped 69095\n",
      "dropped 69337\n",
      "dropped 69405\n",
      "dropped 69406\n",
      "dropped 69407\n",
      "dropped 69475\n",
      "dropped 69495\n",
      "dropped 69496\n",
      "dropped 69497\n",
      "dropped 69571\n",
      "dropped 69613\n",
      "dropped 69631\n",
      "dropped 69663\n",
      "dropped 69664\n",
      "dropped 69665\n",
      "dropped 69795\n",
      "dropped 69796\n",
      "dropped 69797\n",
      "dropped 69951\n",
      "dropped 69952\n",
      "dropped 69953\n",
      "dropped 70317\n",
      "dropped 70318\n",
      "dropped 70319\n",
      "dropped 70371\n",
      "dropped 70372\n",
      "dropped 70373\n",
      "dropped 70665\n",
      "dropped 70666\n",
      "dropped 70667\n",
      "dropped 71365\n",
      "dropped 71541\n",
      "dropped 71542\n",
      "dropped 71543\n",
      "dropped 71929\n",
      "dropped 71961\n",
      "dropped 71962\n",
      "dropped 71963\n",
      "dropped 72091\n",
      "dropped 72092\n",
      "dropped 72321\n",
      "dropped 72322\n",
      "dropped 72323\n",
      "dropped 72369\n",
      "dropped 72370\n",
      "dropped 72371\n",
      "dropped 72885\n",
      "dropped 72886\n",
      "dropped 72887\n",
      "dropped 72957\n",
      "dropped 72958\n",
      "dropped 72959\n",
      "dropped 73021\n",
      "dropped 73069\n",
      "dropped 73233\n",
      "dropped 73234\n",
      "dropped 73235\n",
      "dropped 73243\n",
      "dropped 73387\n",
      "dropped 73521\n",
      "dropped 73522\n",
      "dropped 73523\n",
      "dropped 73657\n",
      "dropped 73761\n",
      "dropped 73762\n",
      "dropped 73763\n",
      "dropped 73855\n",
      "dropped 73971\n",
      "dropped 73972\n",
      "dropped 73973\n",
      "dropped 74421\n",
      "dropped 74422\n",
      "dropped 74423\n",
      "Total Dropped: 686\n"
     ]
    }
   ],
   "source": [
    "# Cleaning data\n",
    "## Remove rows with missing values\n",
    "hasnan=train_set.isna().any(axis=1)\n",
    "droppedcount = 0\n",
    "for i in hasnan.index:\n",
    "    if hasnan[i]:\n",
    "        print(\"dropped\",i)\n",
    "        droppedcount += 1\n",
    "        train_set=train_set.drop(i)\n",
    "print(\"Total Dropped:\",droppedcount)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "82d4c2ef-a858-4660-964e-4465b3ecc3b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "d057163d-fc3d-4343-b652-011b8177ec03",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_len = 128\n",
    "X_train_enc = tokenizer.batch_encode_plus(train_set[\"Tweet Content\"],padding=True,truncation=True,max_length=max_len,return_tensors=\"pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "575a1e3b-5f12-45e2-8ec1-046c68e94611",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "87fe3112-6a62-4716-b396-d9633c90165d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Comments -->> Deep Grounded almost looked pretty cool even despite the borderlands fifth tier unfunny writing format until ultimately it became yet another survival crafting mobile game. I really can â€™ t wait for this shitty trend starting to... die\n",
      "\n",
      "Input Ids -->>\n",
      " tensor([  101, 16764,  2471,  2001,  3492,  4658,  2130,  2750,  1996,  2327,\n",
      "         7563,  4895, 11263, 10695,  2100,  3015,  2127,  2057,  2150,  2664,\n",
      "         2178, 15703,  7477,  2075,  2208,  1012,  1045,  5667,  2064,  1521,\n",
      "         1056,  3524,  2006,  2023, 28543,  9874,  2000,  3280,   102,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0])\n",
      "\n",
      "Decoded Ids -->>\n",
      " [CLS] grounded almost was pretty cool even despite the top tier unfunny writing until we became yet another annoying crafting game. i seriously can â€™ t wait on this shitty trend to die [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]\n",
      "\n",
      "Attention Mask -->>\n",
      " tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0])\n",
      "\n",
      "Labels -->> Negative\n"
     ]
    }
   ],
   "source": [
    "k = 100\n",
    "print('Training Comments -->>',train_set[\"Tweet Content\"][k])\n",
    "print('\\nInput Ids -->>\\n',X_train_enc['input_ids'][k])\n",
    "print('\\nDecoded Ids -->>\\n',tokenizer.decode(X_train_enc['input_ids'][k]))\n",
    "print('\\nAttention Mask -->>\\n',X_train_enc['attention_mask'][k])\n",
    "print('\\nLabels -->>',train_set[\"Sentiment\"][k])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fcfa0f5-e91e-48a4-ab6c-9e46738f18f0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6262b91b-0d92-4d8d-8eae-2f188bdc74e9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "9d130d0e-cf30-4646-af2a-05150845e68e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model = BertForSequenceClassification.from_pretrained('bert-base-uncased',num_labels=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "d288dbda-947f-4ac6-80fe-731f47cea333",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = tf.keras.optimizers.Adam(learning_rate=2e-5)\n",
    "loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "metric = tf.keras.metrics.SparseCategoricalAccuracy('accuracy')\n",
    "model.compile()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "f82c85ca-64dc-4e3e-be8f-a7f0a348e342",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "[enforce fail at alloc_cpu.cpp:124] err == 0. DefaultCPUAllocator: can't allocate memory: you tried to allocate 29096411136 bytes. Error code 12 (Cannot allocate memory)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[55], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m model(input_ids \u001b[38;5;241m=\u001b[39m X_train_enc[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124minput_ids\u001b[39m\u001b[38;5;124m'\u001b[39m], labels\u001b[38;5;241m=\u001b[39mtrain_set[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSentiment\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/site-packages/torch/nn/modules/module.py:1773\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1771\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_wrapped_call_impl\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m   1772\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 1773\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1774\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1775\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/site-packages/torch/_dynamo/eval_frame.py:832\u001b[0m, in \u001b[0;36m_TorchDynamoContext.__call__.<locals>.compile_wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    829\u001b[0m _maybe_set_eval_frame(_callback_from_stance(callback))\n\u001b[1;32m    831\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 832\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    833\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m Unsupported \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    834\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m config\u001b[38;5;241m.\u001b[39mverbose:\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/site-packages/torch/nn/modules/module.py:1786\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1781\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1782\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1783\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1784\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1785\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1786\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1788\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1789\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/site-packages/transformers/models/bert/modeling_bert.py:1460\u001b[0m, in \u001b[0;36mBertForSequenceClassification.forward\u001b[0;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, labels, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1457\u001b[0m     \u001b[38;5;66;03m# Initialize weights and apply final processing\u001b[39;00m\n\u001b[1;32m   1458\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpost_init()\n\u001b[0;32m-> 1460\u001b[0m \u001b[38;5;129m@auto_docstring\u001b[39m\n\u001b[1;32m   1461\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\n\u001b[1;32m   1462\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   1463\u001b[0m     input_ids: Optional[torch\u001b[38;5;241m.\u001b[39mTensor] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   1464\u001b[0m     attention_mask: Optional[torch\u001b[38;5;241m.\u001b[39mTensor] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   1465\u001b[0m     token_type_ids: Optional[torch\u001b[38;5;241m.\u001b[39mTensor] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   1466\u001b[0m     position_ids: Optional[torch\u001b[38;5;241m.\u001b[39mTensor] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   1467\u001b[0m     head_mask: Optional[torch\u001b[38;5;241m.\u001b[39mTensor] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   1468\u001b[0m     inputs_embeds: Optional[torch\u001b[38;5;241m.\u001b[39mTensor] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   1469\u001b[0m     labels: Optional[torch\u001b[38;5;241m.\u001b[39mTensor] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   1470\u001b[0m     output_attentions: Optional[\u001b[38;5;28mbool\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   1471\u001b[0m     output_hidden_states: Optional[\u001b[38;5;28mbool\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   1472\u001b[0m     return_dict: Optional[\u001b[38;5;28mbool\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   1473\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Union[\u001b[38;5;28mtuple\u001b[39m[torch\u001b[38;5;241m.\u001b[39mTensor], SequenceClassifierOutput]:\n\u001b[1;32m   1474\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1475\u001b[0m \u001b[38;5;124;03m    labels (`torch.LongTensor` of shape `(batch_size,)`, *optional*):\u001b[39;00m\n\u001b[1;32m   1476\u001b[0m \u001b[38;5;124;03m        Labels for computing the sequence classification/regression loss. Indices should be in `[0, ...,\u001b[39;00m\n\u001b[1;32m   1477\u001b[0m \u001b[38;5;124;03m        config.num_labels - 1]`. If `config.num_labels == 1` a regression loss is computed (Mean-Square loss), If\u001b[39;00m\n\u001b[1;32m   1478\u001b[0m \u001b[38;5;124;03m        `config.num_labels > 1` a classification loss is computed (Cross-Entropy).\u001b[39;00m\n\u001b[1;32m   1479\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m   1480\u001b[0m     return_dict \u001b[38;5;241m=\u001b[39m return_dict \u001b[38;5;28;01mif\u001b[39;00m return_dict \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39muse_return_dict\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/site-packages/torch/_dynamo/eval_frame.py:1044\u001b[0m, in \u001b[0;36mDisableContext.__call__.<locals>._fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m   1042\u001b[0m _maybe_set_eval_frame(_callback_from_stance(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcallback))\n\u001b[1;32m   1043\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1044\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1045\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m   1046\u001b[0m     set_eval_frame(\u001b[38;5;28;01mNone\u001b[39;00m)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/site-packages/torch/_functorch/aot_autograd.py:1130\u001b[0m, in \u001b[0;36maot_module_simplified.<locals>.forward\u001b[0;34m(*runtime_args)\u001b[0m\n\u001b[1;32m   1128\u001b[0m full_args\u001b[38;5;241m.\u001b[39mextend(params_buffers_flat)\n\u001b[1;32m   1129\u001b[0m full_args\u001b[38;5;241m.\u001b[39mextend(runtime_args)\n\u001b[0;32m-> 1130\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m compiled_fn(full_args)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/site-packages/torch/_functorch/_aot_autograd/runtime_wrappers.py:339\u001b[0m, in \u001b[0;36m_create_runtime_wrapper.<locals>.runtime_wrapper\u001b[0;34m(args)\u001b[0m\n\u001b[1;32m    334\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m (\n\u001b[1;32m    335\u001b[0m         torch\u001b[38;5;241m.\u001b[39mautograd\u001b[38;5;241m.\u001b[39m_force_original_view_tracking(\u001b[38;5;28;01mTrue\u001b[39;00m),\n\u001b[1;32m    336\u001b[0m         torch\u001b[38;5;241m.\u001b[39menable_grad(),\n\u001b[1;32m    337\u001b[0m     ):\n\u001b[1;32m    338\u001b[0m         record_runtime_wrapper_prologue_exit(cm)\n\u001b[0;32m--> 339\u001b[0m         all_outs \u001b[38;5;241m=\u001b[39m call_func_at_runtime_with_args(\n\u001b[1;32m    340\u001b[0m             compiled_fn, args_, disable_amp\u001b[38;5;241m=\u001b[39mdisable_amp, steal_args\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m    341\u001b[0m         )\n\u001b[1;32m    342\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    343\u001b[0m     \u001b[38;5;66;03m# When we have an inference graph, we run with grad disabled.\u001b[39;00m\n\u001b[1;32m    344\u001b[0m     \u001b[38;5;66;03m# It's possible to get an inference graph with inputs that require grad,\u001b[39;00m\n\u001b[1;32m    345\u001b[0m     \u001b[38;5;66;03m# in which case we want to make sure autograd is disabled\u001b[39;00m\n\u001b[1;32m    346\u001b[0m     \u001b[38;5;66;03m# (since e.g., inductor will generate aten.addmm.out calls which autograd will complain on)\u001b[39;00m\n\u001b[1;32m    347\u001b[0m     \u001b[38;5;66;03m# NOTE: We use _set_grad_enabled directly to reduce runtime overhead\u001b[39;00m\n\u001b[1;32m    348\u001b[0m     grad_enabled \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mis_grad_enabled()\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/site-packages/torch/_functorch/_aot_autograd/utils.py:129\u001b[0m, in \u001b[0;36mcall_func_at_runtime_with_args\u001b[0;34m(f, args, steal_args, disable_amp)\u001b[0m\n\u001b[1;32m    127\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m context():\n\u001b[1;32m    128\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(f, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_boxed_call\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[0;32m--> 129\u001b[0m         out \u001b[38;5;241m=\u001b[39m normalize_as_list(f(args))\n\u001b[1;32m    130\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    131\u001b[0m         \u001b[38;5;66;03m# TODO: Please remove soon\u001b[39;00m\n\u001b[1;32m    132\u001b[0m         \u001b[38;5;66;03m# https://github.com/pytorch/pytorch/pull/83137#issuecomment-1211320670\u001b[39;00m\n\u001b[1;32m    133\u001b[0m         warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[1;32m    134\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYour compiler for AOTAutograd is returning a function that doesn\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt take boxed arguments. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    135\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPlease wrap it with functorch.compile.make_boxed_func or handle the boxed arguments yourself. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    136\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSee https://github.com/pytorch/pytorch/pull/83137#issuecomment-1211320670 for rationale.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    137\u001b[0m         )\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/site-packages/torch/_functorch/_aot_autograd/utils.py:103\u001b[0m, in \u001b[0;36mmake_boxed_func.<locals>.g\u001b[0;34m(args)\u001b[0m\n\u001b[1;32m    102\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mg\u001b[39m(args):\n\u001b[0;32m--> 103\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m f(\u001b[38;5;241m*\u001b[39margs)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/site-packages/torch/autograd/function.py:581\u001b[0m, in \u001b[0;36mFunction.apply\u001b[0;34m(cls, *args, **kwargs)\u001b[0m\n\u001b[1;32m    578\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_C\u001b[38;5;241m.\u001b[39m_are_functorch_transforms_active():\n\u001b[1;32m    579\u001b[0m     \u001b[38;5;66;03m# See NOTE: [functorch vjp and autograd interaction]\u001b[39;00m\n\u001b[1;32m    580\u001b[0m     args \u001b[38;5;241m=\u001b[39m _functorch\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39munwrap_dead_wrappers(args)\n\u001b[0;32m--> 581\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39mapply(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m    583\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_setup_ctx_defined:\n\u001b[1;32m    584\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m    585\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIn order to use an autograd.Function with functorch transforms \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    586\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m(vmap, grad, jvp, jacrev, ...), it must override the setup_context \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    587\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstaticmethod. For more details, please see \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    588\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhttps://pytorch.org/docs/main/notes/extending.func.html\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    589\u001b[0m     )\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/site-packages/torch/_functorch/_aot_autograd/runtime_wrappers.py:2118\u001b[0m, in \u001b[0;36mAOTDispatchAutograd.post_compile.<locals>.CompiledFunction.forward\u001b[0;34m(ctx, *deduped_flat_tensor_args)\u001b[0m\n\u001b[1;32m   2109\u001b[0m     args \u001b[38;5;241m=\u001b[39m (\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39mfwd_rng_states)\n\u001b[1;32m   2111\u001b[0m \u001b[38;5;66;03m# There is a pretty complicated calling convention around what the compiled fw returns.\u001b[39;00m\n\u001b[1;32m   2112\u001b[0m \u001b[38;5;66;03m# The full list of outputs and their relative order is:\u001b[39;00m\n\u001b[1;32m   2113\u001b[0m \u001b[38;5;66;03m# (*tokens, *mutated_inputs, *fw_outs, *fw_intermediate_bases, *saved_tensors, *saved_symints)\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   2116\u001b[0m \u001b[38;5;66;03m# - Note that donated buffer logic requires (*saved_tensors, *saved_symints) showing up last\u001b[39;00m\n\u001b[1;32m   2117\u001b[0m \u001b[38;5;66;03m#   in the fw output order.\u001b[39;00m\n\u001b[0;32m-> 2118\u001b[0m fw_outs \u001b[38;5;241m=\u001b[39m call_func_at_runtime_with_args(\n\u001b[1;32m   2119\u001b[0m     CompiledFunction\u001b[38;5;241m.\u001b[39mcompiled_fw,\n\u001b[1;32m   2120\u001b[0m     args,\n\u001b[1;32m   2121\u001b[0m     disable_amp\u001b[38;5;241m=\u001b[39mdisable_amp,\n\u001b[1;32m   2122\u001b[0m )\n\u001b[1;32m   2124\u001b[0m num_outputs \u001b[38;5;241m=\u001b[39m CompiledFunction\u001b[38;5;241m.\u001b[39mmetadata\u001b[38;5;241m.\u001b[39mnum_outputs\n\u001b[1;32m   2125\u001b[0m num_outputs_aliased \u001b[38;5;241m=\u001b[39m CompiledFunction\u001b[38;5;241m.\u001b[39mmetadata\u001b[38;5;241m.\u001b[39mnum_outputs_aliased\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/site-packages/torch/_functorch/_aot_autograd/utils.py:129\u001b[0m, in \u001b[0;36mcall_func_at_runtime_with_args\u001b[0;34m(f, args, steal_args, disable_amp)\u001b[0m\n\u001b[1;32m    127\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m context():\n\u001b[1;32m    128\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(f, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_boxed_call\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[0;32m--> 129\u001b[0m         out \u001b[38;5;241m=\u001b[39m normalize_as_list(f(args))\n\u001b[1;32m    130\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    131\u001b[0m         \u001b[38;5;66;03m# TODO: Please remove soon\u001b[39;00m\n\u001b[1;32m    132\u001b[0m         \u001b[38;5;66;03m# https://github.com/pytorch/pytorch/pull/83137#issuecomment-1211320670\u001b[39;00m\n\u001b[1;32m    133\u001b[0m         warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[1;32m    134\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYour compiler for AOTAutograd is returning a function that doesn\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt take boxed arguments. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    135\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPlease wrap it with functorch.compile.make_boxed_func or handle the boxed arguments yourself. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    136\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSee https://github.com/pytorch/pytorch/pull/83137#issuecomment-1211320670 for rationale.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    137\u001b[0m         )\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/site-packages/torch/_functorch/_aot_autograd/runtime_wrappers.py:526\u001b[0m, in \u001b[0;36mFunctionalizedRngRuntimeWrapper.post_compile.<locals>.wrapper\u001b[0;34m(runtime_args)\u001b[0m\n\u001b[1;32m    519\u001b[0m     out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_functionalized_rng_runtime_epilogue(\n\u001b[1;32m    520\u001b[0m         runtime_metadata,\n\u001b[1;32m    521\u001b[0m         out,\n\u001b[1;32m    522\u001b[0m         \u001b[38;5;66;03m# TODO: this won't be right for the backward when we convert the call_compiled_backward to use the wrapper\u001b[39;00m\n\u001b[1;32m    523\u001b[0m         runtime_metadata\u001b[38;5;241m.\u001b[39mnum_forward_returns,\n\u001b[1;32m    524\u001b[0m     )\n\u001b[1;32m    525\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m out\n\u001b[0;32m--> 526\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m compiled_fn(runtime_args)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/site-packages/torch/_functorch/_aot_autograd/runtime_wrappers.py:724\u001b[0m, in \u001b[0;36mEffectTokensWrapper.post_compile.<locals>.inner_fn\u001b[0;34m(args)\u001b[0m\n\u001b[1;32m    721\u001b[0m     args \u001b[38;5;241m=\u001b[39m [\u001b[38;5;241m*\u001b[39m([\u001b[38;5;28;01mNone\u001b[39;00m] \u001b[38;5;241m*\u001b[39m num_tokens), \u001b[38;5;241m*\u001b[39margs]\n\u001b[1;32m    722\u001b[0m     old_args\u001b[38;5;241m.\u001b[39mclear()\n\u001b[0;32m--> 724\u001b[0m outs \u001b[38;5;241m=\u001b[39m compiled_fn(args)\n\u001b[1;32m    726\u001b[0m \u001b[38;5;66;03m# Inductor cache DummyModule can return None\u001b[39;00m\n\u001b[1;32m    727\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m outs \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/site-packages/torch/_inductor/output_code.py:613\u001b[0m, in \u001b[0;36mCompiledFxGraph.__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m    609\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    610\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m record_function(\n\u001b[1;32m    611\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m## Call CompiledFxGraph \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fx_graph_cache_key\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m ##\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    612\u001b[0m     ):\n\u001b[0;32m--> 613\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcurrent_callable(inputs)\n\u001b[1;32m    614\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    615\u001b[0m     get_runtime_metrics_context()\u001b[38;5;241m.\u001b[39mfinish()\n",
      "File \u001b[0;32m/tmp/torchinductor_liam/ej/cejunk4iumcfelssb2nkbxjl6ggws3hzqop4xlkpgauboutn3xdx.py:3440\u001b[0m, in \u001b[0;36mRunner.call\u001b[0;34m(self, args)\u001b[0m\n\u001b[1;32m   3438\u001b[0m assert_size_stride(primals_203, (\u001b[38;5;241m3\u001b[39m, \u001b[38;5;241m768\u001b[39m), (\u001b[38;5;241m768\u001b[39m, \u001b[38;5;241m1\u001b[39m))\n\u001b[1;32m   3439\u001b[0m assert_size_stride(primals_204, (\u001b[38;5;241m3\u001b[39m, ), (\u001b[38;5;241m1\u001b[39m, ))\n\u001b[0;32m-> 3440\u001b[0m buf0 \u001b[38;5;241m=\u001b[39m empty_strided_cpu((\u001b[38;5;241m73996\u001b[39m, \u001b[38;5;241m128\u001b[39m, \u001b[38;5;241m768\u001b[39m), (\u001b[38;5;241m98304\u001b[39m, \u001b[38;5;241m768\u001b[39m, \u001b[38;5;241m1\u001b[39m), torch\u001b[38;5;241m.\u001b[39mfloat32)\n\u001b[1;32m   3441\u001b[0m buf1 \u001b[38;5;241m=\u001b[39m empty_strided_cpu((\u001b[38;5;241m73996\u001b[39m, \u001b[38;5;241m128\u001b[39m, \u001b[38;5;241m1\u001b[39m), (\u001b[38;5;241m128\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m9471488\u001b[39m), torch\u001b[38;5;241m.\u001b[39mfloat32)\n\u001b[1;32m   3442\u001b[0m buf2 \u001b[38;5;241m=\u001b[39m empty_strided_cpu((\u001b[38;5;241m73996\u001b[39m, \u001b[38;5;241m128\u001b[39m, \u001b[38;5;241m1\u001b[39m), (\u001b[38;5;241m128\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m9471488\u001b[39m), torch\u001b[38;5;241m.\u001b[39mfloat32)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: [enforce fail at alloc_cpu.cpp:124] err == 0. DefaultCPUAllocator: can't allocate memory: you tried to allocate 29096411136 bytes. Error code 12 (Cannot allocate memory)"
     ]
    }
   ],
   "source": [
    "model(input_ids = X_train_enc['input_ids'], labels=train_set[\"Sentiment\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "733be42d-d809-4b7e-b72e-a4b9f45d99e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Device set to use cpu\n"
     ]
    }
   ],
   "source": [
    "model_p = pipeline(task='sentiment-analysis',model='bert-base-uncased',tokenizer='bert-base-uncased',framework='pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "4851b120-1146-435d-9c82-adfac9543deb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'label': 'LABEL_0', 'score': 0.6496325135231018}]\n",
      "Negative\n",
      "I'll just say it - being nominated for the LGBTQ + game of the year is essentially worse than being nominated for the LGBTQ + movie of the year for Rise of Skywalker. At least in Rise of Skywalker, if you slow down and zoom in, you can see homosexual shit when you pinch yourself.\n"
     ]
    }
   ],
   "source": [
    "k = 7777\n",
    "print(model_p.predict(train_set[\"Tweet Content\"][k]))\n",
    "print(train_set[\"Sentiment\"][k])\n",
    "print(train_set[\"Tweet Content\"][k])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21c0eef6-3c4d-40c5-a81b-7e0f5889fee5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
