{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b60d71f1-6dc8-4bf9-a29d-89e4203e2323",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-10 14:22:53.263124: I external/local_xla/xla/tsl/cuda/cudart_stub.cc:31] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2025-11-10 14:22:53.359032: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2025-11-10 14:22:55.515475: I external/local_xla/xla/tsl/cuda/cudart_stub.cc:31] Could not find cuda drivers on your machine, GPU will not be used.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, BertForMultipleChoice\n",
    "import pandas as pd\n",
    "#from transformers import pipeline\n",
    "from tqdm import tqdm\n",
    "from datasets import load_dataset\n",
    "\n",
    "# restrict trainer to not use all cores\n",
    "import os\n",
    "os.environ[\"OPENMP_NUM_THREADS\"] = \"1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d84a0e05-2143-478f-8d4c-47966bdc071e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.environ[\"OPENMP_NUM_THREADS\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a6021928-00c9-4053-8f6b-8004bbdf5585",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['Unnamed: 0.1', 'Unnamed: 0', 'Tweet Id', 'Entity', 'Sentiment', 'Tweet Content'],\n",
       "        num_rows: 160028\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['Unnamed: 0.1', 'Unnamed: 0', 'Tweet Id', 'Entity', 'Sentiment', 'Tweet Content'],\n",
       "        num_rows: 1500\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['Unnamed: 0.1', 'Unnamed: 0', 'Tweet Id', 'Entity', 'Sentiment', 'Tweet Content'],\n",
       "        num_rows: 1000\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = load_dataset(\"../data/clean\")\n",
    "                       #names=[\"Tweet Id\",\"Entity\",\"Sentiment\",\"Tweet Content\"]\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c529e170-e765-414b-a1cb-0bec2568d3a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoding sentiment\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train: 100%|█████████████████████████████████████████████████████████████| 160028/160028 [00:05<00:00, 31795.09it/s]\n",
      "validation: 100%|████████████████████████████████████████████████████████████| 1500/1500 [00:00<00:00, 24126.36it/s]\n",
      "test: 100%|██████████████████████████████████████████████████████████████████| 1000/1000 [00:00<00:00, 29672.13it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Combining entities and text\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train: 100%|███████████████████████████████████████████████████████████████| 160028/160028 [06:17<00:00, 423.51it/s]\n",
      "validation: 100%|██████████████████████████████████████████████████████████████| 1500/1500 [00:01<00:00, 754.68it/s]\n",
      "test: 100%|████████████████████████████████████████████████████████████████████| 1000/1000 [00:01<00:00, 825.16it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['Sentiment', 'labels', 'inputs'],\n",
       "        num_rows: 160028\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['Sentiment', 'labels', 'inputs'],\n",
       "        num_rows: 1500\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['Sentiment', 'labels', 'inputs'],\n",
       "        num_rows: 1000\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = dataset.remove_columns(['Unnamed: 0.1', 'Unnamed: 0', 'Tweet Id'])\n",
    "#dataset = dataset.\n",
    "\n",
    "sentiments = [\"Irrelevant\",\"Positive\",\"Neutral\",\"Negative\"]\n",
    "\n",
    "print(\"Encoding sentiment\")\n",
    "\n",
    "for subset in [\"train\",\"validation\",\"test\"]:\n",
    "    labels = []\n",
    "    for d in tqdm(dataset[subset][\"Sentiment\"], desc=subset):\n",
    "        for s in range(len(sentiments)):\n",
    "            if sentiments[s] == d:\n",
    "                labels.append(s)\n",
    "                break\n",
    "    dataset[subset] = dataset[subset].add_column(\"labels\",labels)\n",
    "\n",
    "print(\"Combining entities and text\")\n",
    "for subset in [\"train\",\"validation\",\"test\"]:\n",
    "    combined = []\n",
    "    for d in tqdm(range(len(dataset[subset])), desc=subset):\n",
    "        combined.append(dataset[subset][\"Entity\"][d] + \". \" + dataset[subset][\"Tweet Content\"][d])\n",
    "    dataset[subset] = dataset[subset].add_column(\"inputs\",combined)\n",
    "\n",
    "dataset = dataset.remove_columns([\"Entity\",\"Tweet Content\"])\n",
    "\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4c624c0e-9ce3-429b-b51e-d8f42b995c91",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b23c940256c14f3b9621dedf1c1d1758",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/160028 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a2ed4e991ad743d8ac4450dc39d63791",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/1500 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "32a5a4e98fe44d04b83529f976109fcb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/1000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['Sentiment', 'labels', 'inputs'],\n",
       "        num_rows: 160028\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['Sentiment', 'labels', 'inputs'],\n",
       "        num_rows: 1500\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['Sentiment', 'labels', 'inputs'],\n",
       "        num_rows: 1000\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#dataset = dataset.remove_columns([\"Sentiment\"])\n",
    "dataset.save_to_disk(\"bert-ds\")\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6b779b6d-3ce7-4d13-a2a7-accbb8a50f2e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Sentiment': ['Negative', 'Neutral', 'Negative', 'Neutral'],\n",
       " 'labels': [3, 2, 3, 2],\n",
       " 'inputs': ['CallOfDuty. ... This is chilling',\n",
       "  'Google. one',\n",
       "  'ApexLegends. literally toxic bro, came out of the game when I was clear to him. Oh and I hit the same person three times to talk about lmao disappointment.',\n",
       "  'WorldOfCraft. Take a look at this article I just got! [Uncanny Combat Gloves of the Incomparable Fighter]']}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[\"train\"][0:4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "406d7ef8-800b-482a-9274-f25cd906677e",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"google-bert/bert-base-uncased\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "47b7f97a-dc43-4a58-b794-882fba09853f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# adapted from https://huggingface.co/docs/transformers/tasks/multiple_choice\n",
    "\n",
    "count = 0\n",
    "total = int(len(dataset[\"train\"])/1000)\n",
    "\n",
    "def preprocess(examples):\n",
    "    global count, total\n",
    "    count += 1\n",
    "    tweet_text = [[content] * 4 for content in examples[\"inputs\"]]\n",
    "    #entities = examples[\"Entity\"]\n",
    "    \n",
    "    tweet_sentiment = [\n",
    "        [*sentiments] for label in examples[\"labels\"]\n",
    "    ]\n",
    "\n",
    "    tweet_text = sum(tweet_text, [])\n",
    "    tweet_sentiment = sum(tweet_sentiment, [])\n",
    "\n",
    "    print(count,\"/\",total,\"\\t\", len(tweet_text), len(tweet_sentiment))\n",
    "\n",
    "    tokenized_examples = tokenizer(tweet_text, tweet_sentiment, truncation = True)\n",
    "    return {k: [v[i : i+4] for i in range(0,len(v),4)] for k, v in tokenized_examples.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2902b3a5-7354-485a-9b10-cdae6659e114",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f1f7cb23134d4f4798cb33bb855b36a0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/160028 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 / 160 \t 4000 4000\n",
      "2 / 160 \t 4000 4000\n",
      "3 / 160 \t 4000 4000\n",
      "4 / 160 \t 4000 4000\n",
      "5 / 160 \t 4000 4000\n",
      "6 / 160 \t 4000 4000\n",
      "7 / 160 \t 4000 4000\n",
      "8 / 160 \t 4000 4000\n",
      "9 / 160 \t 4000 4000\n",
      "10 / 160 \t 4000 4000\n",
      "11 / 160 \t 4000 4000\n",
      "12 / 160 \t 4000 4000\n",
      "13 / 160 \t 4000 4000\n",
      "14 / 160 \t 4000 4000\n",
      "15 / 160 \t 4000 4000\n",
      "16 / 160 \t 4000 4000\n",
      "17 / 160 \t 4000 4000\n",
      "18 / 160 \t 4000 4000\n",
      "19 / 160 \t 4000 4000\n",
      "20 / 160 \t 4000 4000\n",
      "21 / 160 \t 4000 4000\n",
      "22 / 160 \t 4000 4000\n",
      "23 / 160 \t 4000 4000\n",
      "24 / 160 \t 4000 4000\n",
      "25 / 160 \t 4000 4000\n",
      "26 / 160 \t 4000 4000\n",
      "27 / 160 \t 4000 4000\n",
      "28 / 160 \t 4000 4000\n",
      "29 / 160 \t 4000 4000\n",
      "30 / 160 \t 4000 4000\n",
      "31 / 160 \t 4000 4000\n",
      "32 / 160 \t 4000 4000\n",
      "33 / 160 \t 4000 4000\n",
      "34 / 160 \t 4000 4000\n",
      "35 / 160 \t 4000 4000\n",
      "36 / 160 \t 4000 4000\n",
      "37 / 160 \t 4000 4000\n",
      "38 / 160 \t 4000 4000\n",
      "39 / 160 \t 4000 4000\n",
      "40 / 160 \t 4000 4000\n",
      "41 / 160 \t 4000 4000\n",
      "42 / 160 \t 4000 4000\n",
      "43 / 160 \t 4000 4000\n",
      "44 / 160 \t 4000 4000\n",
      "45 / 160 \t 4000 4000\n",
      "46 / 160 \t 4000 4000\n",
      "47 / 160 \t 4000 4000\n",
      "48 / 160 \t 4000 4000\n",
      "49 / 160 \t 4000 4000\n",
      "50 / 160 \t 4000 4000\n",
      "51 / 160 \t 4000 4000\n",
      "52 / 160 \t 4000 4000\n",
      "53 / 160 \t 4000 4000\n",
      "54 / 160 \t 4000 4000\n",
      "55 / 160 \t 4000 4000\n",
      "56 / 160 \t 4000 4000\n",
      "57 / 160 \t 4000 4000\n",
      "58 / 160 \t 4000 4000\n",
      "59 / 160 \t 4000 4000\n",
      "60 / 160 \t 4000 4000\n",
      "61 / 160 \t 4000 4000\n",
      "62 / 160 \t 4000 4000\n",
      "63 / 160 \t 4000 4000\n",
      "64 / 160 \t 4000 4000\n",
      "65 / 160 \t 4000 4000\n",
      "66 / 160 \t 4000 4000\n",
      "67 / 160 \t 4000 4000\n",
      "68 / 160 \t 4000 4000\n",
      "69 / 160 \t 4000 4000\n",
      "70 / 160 \t 4000 4000\n",
      "71 / 160 \t 4000 4000\n",
      "72 / 160 \t 4000 4000\n",
      "73 / 160 \t 4000 4000\n",
      "74 / 160 \t 4000 4000\n",
      "75 / 160 \t 4000 4000\n",
      "76 / 160 \t 4000 4000\n",
      "77 / 160 \t 4000 4000\n",
      "78 / 160 \t 4000 4000\n",
      "79 / 160 \t 4000 4000\n",
      "80 / 160 \t 4000 4000\n",
      "81 / 160 \t 4000 4000\n",
      "82 / 160 \t 4000 4000\n",
      "83 / 160 \t 4000 4000\n",
      "84 / 160 \t 4000 4000\n",
      "85 / 160 \t 4000 4000\n",
      "86 / 160 \t 4000 4000\n",
      "87 / 160 \t 4000 4000\n",
      "88 / 160 \t 4000 4000\n",
      "89 / 160 \t 4000 4000\n",
      "90 / 160 \t 4000 4000\n",
      "91 / 160 \t 4000 4000\n",
      "92 / 160 \t 4000 4000\n",
      "93 / 160 \t 4000 4000\n",
      "94 / 160 \t 4000 4000\n",
      "95 / 160 \t 4000 4000\n",
      "96 / 160 \t 4000 4000\n",
      "97 / 160 \t 4000 4000\n",
      "98 / 160 \t 4000 4000\n",
      "99 / 160 \t 4000 4000\n",
      "100 / 160 \t 4000 4000\n",
      "101 / 160 \t 4000 4000\n",
      "102 / 160 \t 4000 4000\n",
      "103 / 160 \t 4000 4000\n",
      "104 / 160 \t 4000 4000\n",
      "105 / 160 \t 4000 4000\n",
      "106 / 160 \t 4000 4000\n",
      "107 / 160 \t 4000 4000\n",
      "108 / 160 \t 4000 4000\n",
      "109 / 160 \t 4000 4000\n",
      "110 / 160 \t 4000 4000\n",
      "111 / 160 \t 4000 4000\n",
      "112 / 160 \t 4000 4000\n",
      "113 / 160 \t 4000 4000\n",
      "114 / 160 \t 4000 4000\n",
      "115 / 160 \t 4000 4000\n",
      "116 / 160 \t 4000 4000\n",
      "117 / 160 \t 4000 4000\n",
      "118 / 160 \t 4000 4000\n",
      "119 / 160 \t 4000 4000\n",
      "120 / 160 \t 4000 4000\n",
      "121 / 160 \t 4000 4000\n",
      "122 / 160 \t 4000 4000\n",
      "123 / 160 \t 4000 4000\n",
      "124 / 160 \t 4000 4000\n",
      "125 / 160 \t 4000 4000\n",
      "126 / 160 \t 4000 4000\n",
      "127 / 160 \t 4000 4000\n",
      "128 / 160 \t 4000 4000\n",
      "129 / 160 \t 4000 4000\n",
      "130 / 160 \t 4000 4000\n",
      "131 / 160 \t 4000 4000\n",
      "132 / 160 \t 4000 4000\n",
      "133 / 160 \t 4000 4000\n",
      "134 / 160 \t 4000 4000\n",
      "135 / 160 \t 4000 4000\n",
      "136 / 160 \t 4000 4000\n",
      "137 / 160 \t 4000 4000\n",
      "138 / 160 \t 4000 4000\n",
      "139 / 160 \t 4000 4000\n",
      "140 / 160 \t 4000 4000\n",
      "141 / 160 \t 4000 4000\n",
      "142 / 160 \t 4000 4000\n",
      "143 / 160 \t 4000 4000\n",
      "144 / 160 \t 4000 4000\n",
      "145 / 160 \t 4000 4000\n",
      "146 / 160 \t 4000 4000\n",
      "147 / 160 \t 4000 4000\n",
      "148 / 160 \t 4000 4000\n",
      "149 / 160 \t 4000 4000\n",
      "150 / 160 \t 4000 4000\n",
      "151 / 160 \t 4000 4000\n",
      "152 / 160 \t 4000 4000\n",
      "153 / 160 \t 4000 4000\n",
      "154 / 160 \t 4000 4000\n",
      "155 / 160 \t 4000 4000\n",
      "156 / 160 \t 4000 4000\n",
      "157 / 160 \t 4000 4000\n",
      "158 / 160 \t 4000 4000\n",
      "159 / 160 \t 4000 4000\n",
      "160 / 160 \t 4000 4000\n",
      "161 / 160 \t 112 112\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "662ca08b8d2449859503978f4ed6aabe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1500 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "162 / 160 \t 4000 4000\n",
      "163 / 160 \t 2000 2000\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0d1d73b0ae974412b57611930ea049da",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "164 / 160 \t 4000 4000\n"
     ]
    }
   ],
   "source": [
    "#pbar = tqdm(total=len(dataset))\n",
    "tokenized_data = dataset.map(preprocess, batched = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "9a203330-def5-4486-b020-a5cbe9848fb7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['Sentiment', 'labels', 'inputs', 'input_ids', 'token_type_ids', 'attention_mask'],\n",
       "        num_rows: 160028\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['Sentiment', 'labels', 'inputs', 'input_ids', 'token_type_ids', 'attention_mask'],\n",
       "        num_rows: 1500\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['Sentiment', 'labels', 'inputs', 'input_ids', 'token_type_ids', 'attention_mask'],\n",
       "        num_rows: 1000\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "01f9d006-6fd7-48ad-9bfa-3b1edf5eb765",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DataCollatorForMultipleChoice\n",
    "collator = DataCollatorForMultipleChoice(tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d92a1eeb-5375-49b5-9139-5917cffd26a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import evaluate\n",
    "\n",
    "accuracy = evaluate.load(\"accuracy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "687a9815-7aee-472a-9dca-461549cdf61f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForMultipleChoice were not initialized from the model checkpoint at google-bert/bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForMultipleChoice, TrainingArguments, Trainer\n",
    "\n",
    "model = AutoModelForMultipleChoice.from_pretrained(\"google-bert/bert-base-uncased\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "c7ad2673-9c4a-45d2-8da7-95d157d3274b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    predictions, labels = eval_pred\n",
    "    predictions = np.argmax(predictions, axis=1)\n",
    "    return accuracy.compute(predictions=predictions, references=labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c72d905-a9ab-4fd5-a834-fb9dc632a37e",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2539564-7d35-409b-9585-4ec0aa03b2fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir=\"../models\",\n",
    "    eval_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    load_best_model_at_end=True,\n",
    "    learning_rate=5e-5,\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=16,\n",
    "    num_train_epochs=3,\n",
    "    weight_decay=0.01,\n",
    "    push_to_hub=False,\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_data[\"train\"],\n",
    "    eval_dataset=tokenized_data[\"validation\"],\n",
    "    processing_class=tokenizer,\n",
    "    data_collator=collator,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02001750-2f5f-4581-87ac-aa3cfd4906f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_pretrained(\"../models\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7b54df0-e084-4c0d-9312-2653c1f36c55",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = tokenized_data[\"train\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0945529d-3a56-46c5-9012-c33bd6ff902d",
   "metadata": {},
   "outputs": [],
   "source": [
    "a.rename_column(\"Unnamed: 0\",'labels')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa6d0be5-b82f-48d8-b512-8c968def1420",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_data = tokenized_data.rename_column(\"Unnamed: 0\",'labels')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "210086f5-83dd-421a-afea-163d6b6a3402",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['Sentiment', 'labels', 'inputs', 'input_ids', 'token_type_ids', 'attention_mask'],\n",
       "        num_rows: 160028\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['Sentiment', 'labels', 'inputs', 'input_ids', 'token_type_ids', 'attention_mask'],\n",
       "        num_rows: 1500\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['Sentiment', 'labels', 'inputs', 'input_ids', 'token_type_ids', 'attention_mask'],\n",
       "        num_rows: 1000\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4563a66c-b8e9-45d8-95f6-553ee80a79ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = dataset.rename_column(\"label\",\"labels\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6c462b7-5580-4456-be3d-62afd96e4170",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
